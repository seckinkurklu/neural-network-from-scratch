Neural Network from Scratch (MNIST Digit Recognizer)This project demonstrates the fundamental principles of deep learning by implementing a neural network without the use of high-level frameworks like TensorFlow or PyTorch. The model is trained on the MNIST Digit Recognizer dataset to identify handwritten digits (0-9).üß† Model ArchitectureThe network consists of a simple two-layer architecture:Input Layer: 784 neurons (representing the $28 \times 28$ pixel images).Hidden Layer: 10 neurons using the ReLU activation function.Output Layer: 10 neurons (one for each digit) using the Softmax activation function.üõ†Ô∏è Key ComponentsThe implementation covers the entire machine learning pipeline:Data Preprocessing: Loading CSV data, normalizing pixel values to a range of $[0, 1]$, and splitting the data into training and development sets.Forward Propagation: Computing the dot product of weights and inputs, adding biases, and applying activation functions.Backward Propagation: Calculating gradients using chain rule derivatives to determine how to adjust weights and biases.Gradient Descent: Iteratively updating the parameters ($\text{weight} = \text{weight} - \alpha \times \text{gradient}$) to minimize the loss.üìà PerformanceIn the current configuration, the model achieves the following during training:Initial Accuracy: ~9.9% (random guessing).Accuracy after 500 Iterations: ~85.1%.Learning Rate ($\alpha$): 0.10.üöÄ How to UseEnvironment: Ensure you have Python installed with numpy and pandas.Dataset: The notebook expects the MNIST train.csv file located at /kaggle/input/digit-recognizer/.Execution: Run the cells sequentially to initialize parameters, define helper functions (ReLU, Softmax, etc.), and start the gradient_descent loop.
